"""
LLM ê¸°ë°˜ ê°œì¸ì •ë³´ ë¶„ì„ê¸° (ê°œì¸ì •ë³´ë³´í˜¸ë²• ì¤€ìˆ˜)

ë²•ì  ë¶„ë¥˜ ì²´ê³„:
- ê³ ìœ ì‹ë³„ì •ë³´ (ì œ24ì¡°): ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, ì—¬ê¶Œë²ˆí˜¸, ìš´ì „ë©´í—ˆë²ˆí˜¸, ì™¸êµ­ì¸ë“±ë¡ë²ˆí˜¸
- ë¯¼ê°ì •ë³´ (ì œ23ì¡°): ì‚¬ìƒÂ·ì‹ ë…, ë…¸ë™ì¡°í•©Â·ì •ë‹¹, ì •ì¹˜ì  ê²¬í•´, ê±´ê°•, ì„±ìƒí™œ ë“±
- ê¸ˆìœµì •ë³´ (ì œ34ì¡°ì˜2): ê³„ì¢Œë²ˆí˜¸, ì¹´ë“œë²ˆí˜¸ (ë…¸ì¶œê¸ˆì§€)
- ì¼ë°˜ê°œì¸ì •ë³´ (ì œ2ì¡°): ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ì£¼ì†Œ ë“±
"""
import re
import json
import requests
from typing import List, Dict, Tuple, Optional
from utils.constants import (
    SENSITIVE_PATTERNS, OLLAMA_URL, OLLAMA_TAGS_URL, OLLAMA_TIMEOUT,
    SENSITIVE_KEYWORDS, SEVERITY_WEIGHTS, INFO_LEGAL_CATEGORY,
    LEGAL_CATEGORY_DESCRIPTIONS, UNIQUE_IDENTIFIERS, EXPOSURE_PROHIBITED_INFO,
    CONTEXT_KEYWORDS
)
from utils.logger import logger
from core.recommendation_engine import SecurityRecommendationEngine


class LocalLLMAnalyzer:
    """LLM ë¶„ì„ ì—”ì§„ (ê°œì¸ì •ë³´ë³´í˜¸ë²• ê¸°ë°˜ ë¶„ë¥˜)"""
    
    # íƒì§€ ìš°ì„ ìˆœìœ„ (ë²•ì  ì¤‘ìš”ë„ + íŒ¨í„´ ëª…í™•ì„± ìˆœì„œ)
    # ì¤‘ìš”: íœ´ëŒ€ì „í™”/ì „í™”ë²ˆí˜¸ê°€ ê³„ì¢Œë²ˆí˜¸ë³´ë‹¤ ë¨¼ì € ì™€ì•¼ í•¨ (í˜•ì‹ì´ ë” ëª…í™•)
    PRIORITY_ORDER = [
        # 1ìˆœìœ„: ê³ ìœ ì‹ë³„ì •ë³´ (ì œ24ì¡°) - ê°€ì¥ ì—„ê²©í•œ ë³´í˜¸
        "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸",
        "ì™¸êµ­ì¸ë“±ë¡ë²ˆí˜¸",
        "ì—¬ê¶Œë²ˆí˜¸",
        "ìš´ì „ë©´í—ˆë²ˆí˜¸",
        
        # 2ìˆœìœ„: ëª…í™•í•œ í˜•ì‹ (false positive ì ìŒ)
        "ì¹´ë“œë²ˆí˜¸",       # 16ìë¦¬ 4-4-4-4
        "íœ´ëŒ€ì „í™”",       # 010/011 ë“±ìœ¼ë¡œ ì‹œì‘ - ê³„ì¢Œë²ˆí˜¸ë³´ë‹¤ ë¨¼ì €!
        "ì „í™”ë²ˆí˜¸",       # ì§€ì—­ë²ˆí˜¸ë¡œ ì‹œì‘
        "ì´ë©”ì¼",         # @ í¬í•¨
        
        # 3ìˆœìœ„: ì»¨í…ìŠ¤íŠ¸ ê²€ì¦ í•„ìš” (false positive ê°€ëŠ¥)
        "ê³„ì¢Œë²ˆí˜¸",       # ì»¨í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ ì œì™¸
        "ì£¼ì†Œ",
        "IPì£¼ì†Œ",
    ]
    
    def __init__(self, model_name: str = "llama3.2:3b", status_callback=None):
        self.model_name = model_name
        self.ollama_url = OLLAMA_URL
        self.recommendation_engine = SecurityRecommendationEngine()
        self.status_callback = status_callback
        self.sensitive_types = SENSITIVE_PATTERNS.copy()
    
    def _emit_status(self, message: str):
        """ìƒíƒœ ë©”ì‹œì§€ ì „ì†¡"""
        if self.status_callback:
            self.status_callback(message)
    
    def add_custom_pattern(self, name: str, pattern: str) -> bool:
        """ì»¤ìŠ¤í…€ íŒ¨í„´ ì¶”ê°€"""
        try:
            re.compile(pattern)
            self.sensitive_types[name] = pattern
            return True
        except:
            return False
    
    def check_ollama_connection(self) -> Tuple[bool, str]:
        """Ollama ì—°ê²° í™•ì¸"""
        try:
            response = requests.get(OLLAMA_TAGS_URL, timeout=5)
            if response.status_code == 200:
                models = [m.get('name', '') for m in response.json().get('models', [])]
                if self.model_name in models:
                    return True, f"ì—°ê²° ì„±ê³µ: {self.model_name}"
                return False, f"ëª¨ë¸ ì—†ìŒ. ì‚¬ìš©ê°€ëŠ¥: {', '.join(models[:3])}"
            return False, "Ollama ì„œë²„ ì‘ë‹µ ì—†ìŒ"
        except:
            return False, "Ollamaê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    def _is_overlapping(self, start1: int, end1: int, start2: int, end2: int) -> bool:
        """ë‘ ë²”ìœ„ê°€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸"""
        return not (end1 <= start2 or end2 <= start1)
    
    def _get_legal_category(self, info_type: str) -> str:
        """ì •ë³´ ìœ í˜•ì˜ ë²•ì  ë¶„ë¥˜ ë°˜í™˜"""
        return INFO_LEGAL_CATEGORY.get(info_type, "ì¼ë°˜ê°œì¸ì •ë³´")
    
    def _is_exposure_prohibited(self, info_type: str) -> bool:
        """ë…¸ì¶œê¸ˆì§€ ì •ë³´ ì—¬ë¶€ í™•ì¸ (ì œ34ì¡°ì˜2)"""
        return info_type in EXPOSURE_PROHIBITED_INFO
    
    def detect_sensitive_info_regex(self, text: str) -> List[Dict]:
        """
        ì •ê·œì‹ ê¸°ë°˜ ë¯¼ê°ì •ë³´ íƒì§€ (ì»¨í…ìŠ¤íŠ¸ ê²€ì¦ í¬í•¨)
        
        ê°œì„ ì‚¬í•­:
        1. ì •ê·œì‹ ë§¤ì¹­ í›„ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ í™•ì¸
        2. ê³„ì¢Œë²ˆí˜¸: ì»¨í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ ì œì™¸ (false positive ë°©ì§€)
        3. ì£¼ì†Œ: ì»¨í…ìŠ¤íŠ¸ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ìƒìŠ¹
        """
        detected = []
        detected_ranges = []
        
        # ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ íƒì§€
        for info_type in self.PRIORITY_ORDER:
            pattern = self.sensitive_types.get(info_type)
            if not pattern:
                continue
            
            try:
                for match in re.finditer(
                    pattern, 
                    text, 
                    re.IGNORECASE if info_type == "ì£¼ì†Œ" else 0
                ):
                    start = match.start()
                    end = match.end()
                    value = match.group().strip()
                    
                    # ì¤‘ë³µ ë²”ìœ„ ì²´í¬
                    is_duplicate = False
                    for detected_start, detected_end, detected_type in detected_ranges:
                        if self._is_overlapping(start, end, detected_start, detected_end):
                            is_duplicate = True
                            logger.debug(
                                f"ì¤‘ë³µ ì œì™¸: {info_type} '{value}' "
                                f"(ì´ë¯¸ {detected_type}ë¡œ íƒì§€ë¨)"
                            )
                            break
                    
                    if is_duplicate:
                        continue
                    
                    # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ë’¤ 100ì)
                    context_start = max(0, start - 100)
                    context_end = min(len(text), end + 100)
                    context = text[context_start:context_end].replace('\n', ' ')
                    
                    # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ì¦
                    has_context, confidence = self._validate_with_context(
                        info_type, value, context
                    )
                    
                    # ê³„ì¢Œë²ˆí˜¸ëŠ” ì»¨í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ ì œì™¸ (false positive ë°©ì§€)
                    if info_type == "ê³„ì¢Œë²ˆí˜¸" and not has_context:
                        logger.debug(f"ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ ì œì™¸: {info_type} '{value}'")
                        continue
                    
                    # ë²•ì  ë¶„ë¥˜ ì •ë³´
                    legal_category = self._get_legal_category(info_type)
                    
                    detected.append({
                        'type': info_type,
                        'value': value,
                        'start': start,
                        'end': end,
                        'context': context,
                        'method': 'regex',
                        'confidence': confidence,
                        'legal_category': legal_category,
                        'exposure_prohibited': self._is_exposure_prohibited(info_type),
                        'has_context': has_context
                    })
                    detected_ranges.append((start, end, info_type))
                    logger.debug(f"âœ“ íƒì§€: {info_type} ({legal_category}, {confidence}) - {value[:20]}...")
                    
            except Exception as e:
                logger.error(f"íŒ¨í„´ ë§¤ì¹­ ì˜¤ë¥˜ ({info_type}): {str(e)}")
                continue
        
        detected.sort(key=lambda x: x['start'])
        return detected
    
    def _validate_with_context(self, info_type: str, value: str, context: str) -> Tuple[bool, str]:
        """
        ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ì¦
        
        Args:
            info_type: ì •ë³´ ìœ í˜•
            value: íƒì§€ëœ ê°’
            context: ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            (has_context: bool, confidence: str)
        """
        context_lower = context.lower()
        
        # ê³ ìœ ì‹ë³„ì •ë³´ëŠ” í˜•ì‹ ìì²´ê°€ ëª…í™•í•˜ë¯€ë¡œ í•­ìƒ high
        if info_type in ['ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸', 'ì™¸êµ­ì¸ë“±ë¡ë²ˆí˜¸', 'ì—¬ê¶Œë²ˆí˜¸', 'ìš´ì „ë©´í—ˆë²ˆí˜¸']:
            return True, 'high'
        
        # ì¹´ë“œë²ˆí˜¸ëŠ” 16ìë¦¬ í˜•ì‹ì´ ëª…í™•í•˜ë¯€ë¡œ high
        if info_type == 'ì¹´ë“œë²ˆí˜¸':
            return True, 'high'
        
        # ì´ë©”ì¼ì€ @ í˜•ì‹ì´ ëª…í™•í•˜ë¯€ë¡œ high
        if info_type == 'ì´ë©”ì¼':
            return True, 'high'
        
        # ê³„ì¢Œë²ˆí˜¸: ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ í•„ìˆ˜
        if info_type == 'ê³„ì¢Œë²ˆí˜¸':
            # ê¸¸ì´ ê²€ì¦ (í•˜ì´í”ˆ/ê³µë°± ì œê±° í›„ 10~16ìë¦¬)
            digits_only = re.sub(r'[-\s]', '', value)
            if len(digits_only) < 10 or len(digits_only) > 16:
                return False, 'low'
            
            # ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ í™•ì¸
            keywords = CONTEXT_KEYWORDS.get('ê³„ì¢Œë²ˆí˜¸', [])
            for kw in keywords:
                if kw.lower() in context_lower:
                    return True, 'high'
            
            return False, 'low'
        
        # ì£¼ì†Œ: ì»¨í…ìŠ¤íŠ¸ ìˆìœ¼ë©´ high, ì—†ì–´ë„ medium (í˜•ì‹ì´ ë¹„êµì  ëª…í™•)
        if info_type == 'ì£¼ì†Œ':
            keywords = CONTEXT_KEYWORDS.get('ì£¼ì†Œ', [])
            for kw in keywords:
                if kw.lower() in context_lower:
                    return True, 'high'
            return False, 'medium'
        
        # ì „í™”ë²ˆí˜¸/íœ´ëŒ€ì „í™”: í˜•ì‹ì´ ëª…í™•í•˜ë¯€ë¡œ ê¸°ë³¸ high, ì»¨í…ìŠ¤íŠ¸ ìˆìœ¼ë©´ ë” í™•ì‹¤
        if info_type in ['ì „í™”ë²ˆí˜¸', 'íœ´ëŒ€ì „í™”']:
            keywords = CONTEXT_KEYWORDS.get('ì „í™”ë²ˆí˜¸', [])
            for kw in keywords:
                if kw.lower() in context_lower:
                    return True, 'high'
            return False, 'high'  # ì „í™”ë²ˆí˜¸ í˜•ì‹ ìì²´ê°€ ëª…í™•
        
        # IPì£¼ì†Œ
        if info_type == 'IPì£¼ì†Œ':
            return True, 'medium'
        
        # ê¸°ë³¸ê°’
        return False, 'medium'
    
    def detect_sensitive_keywords(self, text: str) -> List[Dict]:
        """
        ë¯¼ê°ì •ë³´ í‚¤ì›Œë“œ íƒì§€ (ì œ23ì¡°) - ê°œì„ ëœ ë²„ì „
        
        ê°œì¸ì •ë³´ë³´í˜¸ë²• ì œ23ì¡°ì˜ ë¯¼ê°ì •ë³´ëŠ” "íŠ¹ì • ê°œì¸ì— ê´€í•œ" ì •ë³´ì—¬ì•¼ í•¨.
        ë”°ë¼ì„œ ë‹¨ìˆœ í‚¤ì›Œë“œ ì¡´ì¬ê°€ ì•„ë‹Œ, ê°œì¸ê³¼ ì—°ê²°ëœ ë§¥ë½ì¸ì§€ ê²€ì¦ í•„ìš”.
        
        ê°œì„ ì‚¬í•­:
        1. ê°œì¸ ì—°ê²° íŒ¨í„´ í™•ì¸ (ì´ë¦„, ì„±ëª…, í™˜ì, íšŒì› ë“±ê³¼ ì—°ê²°)
        2. ì¸ì ‘ í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ (ì¤‘ë³µ ì¹´ìš´íŠ¸ ë°©ì§€)
        3. ì‹ ë¢°ë„ ì°¨ë“± ì ìš©
        """
        # 1ë‹¨ê³„: ëª¨ë“  í‚¤ì›Œë“œ ìœ„ì¹˜ ìˆ˜ì§‘
        raw_matches = []
        text_lower = text.lower()
        
        for category, keywords in SENSITIVE_KEYWORDS.items():
            for keyword in keywords:
                keyword_lower = keyword.lower()
                start = 0
                while True:
                    pos = text_lower.find(keyword_lower, start)
                    if pos == -1:
                        break
                    
                    raw_matches.append({
                        'category': category,
                        'keyword': keyword,
                        'start': pos,
                        'end': pos + len(keyword),
                        'value': text[pos:pos + len(keyword)]
                    })
                    start = pos + 1
        
        if not raw_matches:
            return []
        
        # 2ë‹¨ê³„: ì¸ì ‘ í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ (50ì ì´ë‚´ = ê°™ì€ ë¬¸ë§¥)
        raw_matches.sort(key=lambda x: x['start'])
        clusters = []
        current_cluster = [raw_matches[0]]
        
        for match in raw_matches[1:]:
            # ì´ì „ í´ëŸ¬ìŠ¤í„°ì˜ ë§ˆì§€ë§‰ í•­ëª©ê³¼ 50ì ì´ë‚´ë©´ ê°™ì€ í´ëŸ¬ìŠ¤í„°
            if match['start'] - current_cluster[-1]['end'] <= 50:
                # ê°™ì€ ì¹´í…Œê³ ë¦¬ê±°ë‚˜ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ë©´ ë³‘í•©
                current_cluster.append(match)
            else:
                clusters.append(current_cluster)
                current_cluster = [match]
        clusters.append(current_cluster)
        
        # 3ë‹¨ê³„: ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ ê°œì¸ ì—°ê²° ì—¬ë¶€ í™•ì¸
        detected = []
        
        for cluster in clusters:
            # í´ëŸ¬ìŠ¤í„° ë²”ìœ„ ê³„ì‚°
            cluster_start = min(m['start'] for m in cluster)
            cluster_end = max(m['end'] for m in cluster)
            
            # í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ë’¤ 150ì)
            context_start = max(0, cluster_start - 150)
            context_end = min(len(text), cluster_end + 150)
            context = text[context_start:context_end].replace('\n', ' ')
            
            # ê°œì¸ ì—°ê²° ì—¬ë¶€ í™•ì¸
            is_personal, connection_type = self._check_personal_connection(context, text, cluster_start)
            
            if is_personal:
                # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ì •ë³´ ìƒì„±
                categories = list(set(m['category'] for m in cluster))
                keywords = list(set(m['keyword'] for m in cluster))
                
                # ëŒ€í‘œ ì¹´í…Œê³ ë¦¬ ê²°ì • (ìš°ì„ ìˆœìœ„: ê±´ê°•ì •ë³´ > ë²”ì£„ê²½ë ¥ > ì‚¬ìƒ_ì‹ ë… > ë‚˜ë¨¸ì§€)
                priority = ['ê±´ê°•ì •ë³´', 'ë²”ì£„ê²½ë ¥', 'ì‚¬ìƒ_ì‹ ë…', 'ë…¸ë™ì¡°í•©_ì •ë‹¹', 'ì„±ìƒí™œ']
                main_category = next((c for c in priority if c in categories), categories[0])
                
                # ê°’ ìƒì„±: ì—°ê²°ëœ í‚¤ì›Œë“œë“¤ì„ í•˜ë‚˜ë¡œ í‘œí˜„
                if len(keywords) == 1:
                    display_value = keywords[0]
                else:
                    display_value = f"{keywords[0]} ì™¸ {len(keywords)-1}ê°œ"
                
                detected.append({
                    'type': main_category,
                    'value': display_value,
                    'start': cluster_start,
                    'end': cluster_end,
                    'context': context,
                    'method': 'keyword',
                    'confidence': 'high' if connection_type == 'direct' else 'medium',
                    'legal_category': 'ë¯¼ê°ì •ë³´',
                    'exposure_prohibited': False,
                    'keywords_matched': keywords,
                    'connection_type': connection_type
                })
        
        return detected
    
    def _check_personal_connection(self, context: str, full_text: str, position: int) -> tuple:
        """
        ë¯¼ê°ì •ë³´ í‚¤ì›Œë“œê°€ íŠ¹ì • ê°œì¸ê³¼ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        
        Returns:
            (is_personal: bool, connection_type: str)
            - connection_type: 'direct' (ì§ì ‘ ì—°ê²°), 'indirect' (ê°„ì ‘ ì—°ê²°), 'none' (ì—°ê²° ì—†ìŒ)
        """
        context_lower = context.lower()
        
        # ì§ì ‘ ì—°ê²° íŒ¨í„´: ê°œì¸ì„ íŠ¹ì •í•˜ëŠ” ëª…í™•í•œ í‘œí˜„
        direct_patterns = [
            # ì¸ì ì‚¬í•­ ë ˆì´ë¸”
            r'ì„±ëª…\s*[:ï¼š]', r'ì´ë¦„\s*[:ï¼š]', r'í™˜ì\s*[:ï¼š]', r'íšŒì›\s*[:ï¼š]',
            r'í”¼ë³´í—˜ì\s*[:ï¼š]', r'ê°€ì…ì\s*[:ï¼š]', r'ì‹ ì²­ì¸\s*[:ï¼š]',
            # ì†Œìœ /ê´€ê³„ í‘œí˜„
            r'[ê°€-í£]{2,4}(ì”¨|ë‹˜|ì˜|ì€|ëŠ”|ì´|ê°€)\s*(ê±´ê°•|ì§„ë‹¨|ë³‘ë ¥|ì¢…êµ|ì‹ ì•™|ì •ë‹¹)',
            r'ë³¸ì¸ì˜?\s*(ê±´ê°•|ì§„ë‹¨|ë³‘ë ¥|ì¢…êµ|ì‹ ì•™|ì •ë‹¹)',
            # ê¸°ë¡ ë¬¸ì„œ í˜•ì‹
            r'(ì§„ë‹¨ì„œ|ì†Œê²¬ì„œ|ì²˜ë°©ì „|ì˜ë¬´ê¸°ë¡|ê±´ê°•ê²€ì§„|ê°€ì…ì‹ ì²­)',
            r'(ì¸ì‚¬ê¸°ë¡|ì‹ ìƒëª…ì„¸|ì´ë ¥ì„œ|ì…ì‚¬ì§€ì›)',
        ]
        
        for pattern in direct_patterns:
            if re.search(pattern, context, re.IGNORECASE):
                return True, 'direct'
        
        # ê°„ì ‘ ì—°ê²°: ë¬¸ì„œ ë‚´ ë‹¤ë¥¸ ê³³ì— ê°œì¸ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
        # ì •ê·œì‹ íƒì§€ëœ ê°œì¸ì •ë³´(ì£¼ë¯¼ë²ˆí˜¸, ì „í™”ë²ˆí˜¸ ë“±)ê°€ ê°™ì€ ë¬¸ì„œì— ìˆìœ¼ë©´
        # ë¯¼ê°ì •ë³´ í‚¤ì›Œë“œë„ ê·¸ ê°œì¸ì— ê´€í•œ ê²ƒì¼ ê°€ëŠ¥ì„± ë†’ìŒ
        personal_indicators = [
            r'\d{6}[-\s]?[1-4]\d{6}',  # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸
            r'01[016789][-\s]?\d{3,4}[-\s]?\d{4}',  # íœ´ëŒ€ì „í™”
            r'ì„±ëª…\s*[:ï¼š]\s*[ê°€-í£]{2,4}',  # ì„±ëª… í•„ë“œ
            r'ì´ë¦„\s*[:ï¼š]\s*[ê°€-í£]{2,4}',  # ì´ë¦„ í•„ë“œ
        ]
        
        # í‚¤ì›Œë“œ ìœ„ì¹˜ ê¸°ì¤€ ì•ë’¤ 500ì ë‚´ì— ê°œì¸ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
        extended_start = max(0, position - 500)
        extended_end = min(len(full_text), position + 500)
        extended_context = full_text[extended_start:extended_end]
        
        for pattern in personal_indicators:
            if re.search(pattern, extended_context):
                return True, 'indirect'
        
        # ë¬¸ì„œ ì „ì²´ê°€ ê°œì¸ì •ë³´ ë¬¸ì„œì¸ì§€ í™•ì¸ (ë¬¸ì„œ ì‹œì‘ ë¶€ë¶„ ì²´í¬)
        doc_header = full_text[:500].lower()
        document_types = [
            'ì¸ì‚¬ê¸°ë¡', 'ì‹ ìƒëª…ì„¸', 'ì´ë ¥ì„œ', 'ì…ì‚¬ì§€ì›', 'ê±´ê°•ê²€ì§„',
            'ì§„ë‹¨ì„œ', 'ì†Œê²¬ì„œ', 'ì²˜ë°©ì „', 'ì˜ë¬´ê¸°ë¡', 'ê°€ì…ì‹ ì²­',
            'ê°œì¸ì •ë³´', 'íšŒì›ì •ë³´', 'í™˜ìì •ë³´', 'ê³ ê°ì •ë³´'
        ]
        
        for doc_type in document_types:
            if doc_type in doc_header:
                return True, 'indirect'
        
        # ì—°ê²° ì—†ìŒ - ì¼ë°˜ì ì¸ ë‹¨ì–´ ì‚¬ìš©ìœ¼ë¡œ íŒë‹¨
        return False, 'none'
    
    def analyze_with_llm(self, text: str) -> Dict:
        """LLM ë¶„ì„ (ê°œì¸ì •ë³´ë³´í˜¸ë²• ê¸°ë°˜)"""
        text_sample = text[:2000]
        
        prompt = f"""ë¬¸ì„œ ë³´ì•ˆ ì „ë¬¸ê°€ë¡œì„œ ê°œì¸ì •ë³´ë³´í˜¸ë²•ì— ë”°ë¼ ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì„¸ìš”.

ã€ë¬¸ì„œã€‘
{text_sample}

ã€ë¶„ë¥˜ ê¸°ì¤€ (ê°œì¸ì •ë³´ë³´í˜¸ë²•)ã€‘
1. ê³ ìœ ì‹ë³„ì •ë³´ (ì œ24ì¡°): ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, ì—¬ê¶Œë²ˆí˜¸, ìš´ì „ë©´í—ˆë²ˆí˜¸, ì™¸êµ­ì¸ë“±ë¡ë²ˆí˜¸
2. ë¯¼ê°ì •ë³´ (ì œ23ì¡°): ì‚¬ìƒÂ·ì‹ ë…, ë…¸ë™ì¡°í•©Â·ì •ë‹¹, ì •ì¹˜ì  ê²¬í•´, ê±´ê°•, ì„±ìƒí™œ ì •ë³´
3. ê¸ˆìœµì •ë³´ (ì œ34ì¡°ì˜2): ê³„ì¢Œë²ˆí˜¸, ì¹´ë“œë²ˆí˜¸ - ë…¸ì¶œ ê¸ˆì§€
4. ì¼ë°˜ê°œì¸ì •ë³´ (ì œ2ì¡°): ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ì£¼ì†Œ ë“±

ã€ìœ„í—˜ë„ ê¸°ì¤€ã€‘
- ë‚®ìŒ: 0-24ì  (ì¼ë°˜ê°œì¸ì •ë³´ ì†ŒëŸ‰)
- ë³´í†µ: 25-49ì  (ì¼ë°˜ê°œì¸ì •ë³´ ë‹¤ìˆ˜ ë˜ëŠ” ë¯¼ê°ì •ë³´ ì†ŒëŸ‰)
- ë†’ìŒ: 50-74ì  (ê³ ìœ ì‹ë³„ì •ë³´ ë˜ëŠ” ê¸ˆìœµì •ë³´ í¬í•¨)
- ì‹¬ê°: 75-100ì  (ê³ ìœ ì‹ë³„ì •ë³´ + ê¸ˆìœµì •ë³´ ë‹¤ìˆ˜, ë³µí•© ë…¸ì¶œ)

ã€ì¶œë ¥ í˜•ì‹ã€‘
ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "detected_info": [{{"type": "ìœ í˜•", "value": "ê°’", "legal_category": "ë²•ì ë¶„ë¥˜"}}],
  "risk_level": "ë‚®ìŒ|ë³´í†µ|ë†’ìŒ|ì‹¬ê°",
  "risk_score": ìˆ«ì(0-100),
  "reasoning": "íƒì§€ëœ ì •ë³´ì™€ ë²•ì  ê·¼ê±°ì— ë”°ë¥¸ ìœ„í—˜ë„ íŒë‹¨",
  "legal_violations": ["ìœ„ë°˜ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì¡°í•­"],
  "recommendations": ["ë³´í˜¸ì¡°ì¹˜1", "ë³´í˜¸ì¡°ì¹˜2", "ë³´í˜¸ì¡°ì¹˜3"]
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            # ì„œë²„ ìƒíƒœ í™•ì¸
            try:
                self._emit_status("ğŸ”— Ollama ì„œë²„ ìƒíƒœ í™•ì¸ ì¤‘...")
                health_response = requests.get(OLLAMA_TAGS_URL, timeout=2)
                if health_response.status_code != 200:
                    logger.warning("Ollama ì„œë²„ ì‘ë‹µ ì—†ìŒ")
                    self._emit_status("âŒ Ollama ì„œë²„ ì‘ë‹µ ì—†ìŒ")
                    return self._create_enhanced_analysis(text)
            except:
                logger.warning("Ollama ì„œë²„ ì ‘ì† ë¶ˆê°€")
                self._emit_status("âŒ Ollama ì„œë²„ ì ‘ì† ë¶ˆê°€")
                return self._create_enhanced_analysis(text)
            
            # LLM í˜¸ì¶œ
            self._emit_status(f"ğŸ¤– {self.model_name} ëª¨ë¸ë¡œ LLM ë¶„ì„ ì¤‘...")
            response = requests.post(
                self.ollama_url,
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "temperature": 0.2,
                    "top_p": 0.9,
                    "top_k": 40
                },
                timeout=OLLAMA_TIMEOUT
            )
            
            if response.status_code == 200:
                self._emit_status("ğŸ“ LLM ì‘ë‹µ íŒŒì‹± ì¤‘...")
                llm_response = response.json().get('response', '')
                parsed = self._parse_json(llm_response)
                
                if parsed and 'recommendations' in parsed:
                    logger.info("LLM ë¶„ì„ ì„±ê³µ")
                    self._emit_status("âœ… LLM ë¶„ì„ ì„±ê³µ")
                    return parsed
                else:
                    logger.warning("LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨")
                    self._emit_status("âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨")
            else:
                logger.warning(f"LLM ì„œë²„ ì˜¤ë¥˜ (status={response.status_code})")
                self._emit_status(f"âŒ LLM ì„œë²„ ì˜¤ë¥˜")
                
        except requests.exceptions.Timeout:
            logger.warning(f"LLM íƒ€ì„ì•„ì›ƒ ({OLLAMA_TIMEOUT}ì´ˆ)")
            self._emit_status(f"â° LLM íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            self._emit_status(f"âŒ LLM ë¶„ì„ ì‹¤íŒ¨")
        
        return self._create_enhanced_analysis(text)
    
    def _parse_json(self, response: str) -> Optional[Dict]:
        """JSON íŒŒì‹±"""
        try:
            return json.loads(response)
        except:
            pass
        
        try:
            start = response.find('{')
            end = response.rfind('}') + 1
            if start != -1 and end > start:
                return json.loads(response[start:end])
        except:
            pass
        
        return None
    
    def _create_enhanced_analysis(self, text: str) -> Dict:
        """ê°•í™”ëœ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ (ê°œì¸ì •ë³´ë³´í˜¸ë²• ì¤€ìˆ˜)"""
        # ì •ê·œì‹ íƒì§€
        regex_detected = self.detect_sensitive_info_regex(text)
        
        # ë¯¼ê°ì •ë³´ í‚¤ì›Œë“œ íƒì§€
        keyword_detected = self.detect_sensitive_keywords(text)
        
        # í†µí•© (ì¤‘ë³µ ì œê±°)
        all_detected = regex_detected.copy()
        existing_ranges = [(d['start'], d['end']) for d in regex_detected]
        
        for kw in keyword_detected:
            is_dup = any(
                self._is_overlapping(kw['start'], kw['end'], s, e)
                for s, e in existing_ranges
            )
            if not is_dup:
                all_detected.append(kw)
                existing_ranges.append((kw['start'], kw['end']))
        
        # ë²•ì  ë¶„ë¥˜ë³„ ì§‘ê³„
        category_counts = {
            "ê³ ìœ ì‹ë³„ì •ë³´": 0,
            "ë¯¼ê°ì •ë³´": 0,
            "ê¸ˆìœµì •ë³´": 0,
            "ì¼ë°˜ê°œì¸ì •ë³´": 0
        }
        
        type_counts = {}
        exposure_prohibited_count = 0
        
        for item in all_detected:
            cat = item.get('legal_category', 'ì¼ë°˜ê°œì¸ì •ë³´')
            category_counts[cat] = category_counts.get(cat, 0) + 1
            
            t = item['type']
            type_counts[t] = type_counts.get(t, 0) + 1
            
            if item.get('exposure_prohibited', False):
                exposure_prohibited_count += 1
        
        # ìœ„í—˜ë„ ê³„ì‚° (ë²•ì  ë¶„ë¥˜ ê¸°ë°˜)
        risk_score = 0
        
        # ê³ ìœ ì‹ë³„ì •ë³´ (ì œ24ì¡°) - ìµœê³  ìœ„í—˜
        unique_id_count = category_counts.get("ê³ ìœ ì‹ë³„ì •ë³´", 0)
        risk_score += unique_id_count * 20
        
        # ê¸ˆìœµì •ë³´ (ì œ34ì¡°ì˜2) - ê³ ìœ„í—˜
        financial_count = category_counts.get("ê¸ˆìœµì •ë³´", 0)
        risk_score += financial_count * 15
        
        # ë¯¼ê°ì •ë³´ (ì œ23ì¡°) - ê³ ìœ„í—˜
        sensitive_count = category_counts.get("ë¯¼ê°ì •ë³´", 0)
        risk_score += sensitive_count * 12
        
        # ì¼ë°˜ê°œì¸ì •ë³´ (ì œ2ì¡°) - ê¸°ë³¸ ìœ„í—˜
        general_count = category_counts.get("ì¼ë°˜ê°œì¸ì •ë³´", 0)
        risk_score += general_count * 5
        
        # ë³µí•© ë…¸ì¶œ ê°€ì¤‘ì¹˜
        active_categories = sum(1 for c in category_counts.values() if c > 0)
        if active_categories >= 3:
            risk_score += 20  # 3ê°œ ì´ìƒ ë¶„ë¥˜ í˜¼ì¬
        elif active_categories >= 2:
            risk_score += 10  # 2ê°œ ë¶„ë¥˜ í˜¼ì¬
        
        # ëŒ€ëŸ‰ ë…¸ì¶œ ê°€ì¤‘ì¹˜
        total_count = len(all_detected)
        if total_count >= 50:
            risk_score += 15
        elif total_count >= 20:
            risk_score += 10
        elif total_count >= 10:
            risk_score += 5
        
        risk_score = min(risk_score, 100)
        
        # ìœ„í—˜ë„ ë ˆë²¨
        if risk_score >= 75:
            risk_level = "ì‹¬ê°"
        elif risk_score >= 50:
            risk_level = "ë†’ìŒ"
        elif risk_score >= 25:
            risk_level = "ë³´í†µ"
        else:
            risk_level = "ë‚®ìŒ"
        
        # ë²•ì  ìœ„ë°˜ ê°€ëŠ¥ì„± íŒë‹¨
        legal_violations = []
        if unique_id_count > 0:
            legal_violations.append("ì œ24ì¡°(ê³ ìœ ì‹ë³„ì •ë³´ ì²˜ë¦¬ì œí•œ) ìœ„ë°˜ ê°€ëŠ¥ì„±")
        if sensitive_count > 0:
            legal_violations.append("ì œ23ì¡°(ë¯¼ê°ì •ë³´ ì²˜ë¦¬ì œí•œ) ìœ„ë°˜ ê°€ëŠ¥ì„±")
        if exposure_prohibited_count > 0:
            legal_violations.append("ì œ34ì¡°ì˜2(ë…¸ì¶œëœ ê°œì¸ì •ë³´ ì‚­ì œÂ·ì°¨ë‹¨) ìœ„ë°˜ ê°€ëŠ¥ì„±")
        
        # íŒë‹¨ ê·¼ê±° ìƒì„±
        reasoning_parts = [f"ì´ {total_count}ê°œì˜ ê°œì¸ì •ë³´ê°€ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤."]
        
        if unique_id_count > 0:
            reasoning_parts.append(
                f"ğŸ”´ ê³ ìœ ì‹ë³„ì •ë³´ {unique_id_count}ê°œ (ì œ24ì¡° - ì²˜ë¦¬ì œí•œ, ì•”í˜¸í™” ì˜ë¬´)"
            )
        if financial_count > 0:
            reasoning_parts.append(
                f"ğŸŸ£ ê¸ˆìœµì •ë³´ {financial_count}ê°œ (ì œ34ì¡°ì˜2 - ë…¸ì¶œê¸ˆì§€)"
            )
        if sensitive_count > 0:
            reasoning_parts.append(
                f"ğŸŸ  ë¯¼ê°ì •ë³´ {sensitive_count}ê°œ (ì œ23ì¡° - ì›ì¹™ì  ì²˜ë¦¬ê¸ˆì§€)"
            )
        if general_count > 0:
            reasoning_parts.append(
                f"ğŸ”µ ì¼ë°˜ê°œì¸ì •ë³´ {general_count}ê°œ (ì œ2ì¡° - ê¸°ë³¸ ë³´í˜¸)"
            )
        
        if active_categories >= 2:
            reasoning_parts.append(
                f"âš ï¸ {active_categories}ê°€ì§€ ë²•ì  ë¶„ë¥˜ì˜ ê°œì¸ì •ë³´ê°€ í˜¼ì¬ë˜ì–´ "
                f"ë³µí•©ì  ìœ„í—˜ë„ê°€ ë†’ìŠµë‹ˆë‹¤."
            )
        
        reasoning = "\n".join(reasoning_parts)
        
        # ê¶Œê³ ì‚¬í•­ ìƒì„±
        recommendations = self.recommendation_engine.generate_recommendations(
            all_detected, risk_level, risk_score, text
        )
        
        return {
            "detected_info": [
                {
                    "type": i['type'], 
                    "value": i['value'], 
                    "context": i['context'],
                    "legal_category": i.get('legal_category', 'ì¼ë°˜ê°œì¸ì •ë³´')
                }
                for i in all_detected
            ],
            "risk_level": risk_level,
            "risk_score": risk_score,
            "reasoning": reasoning,
            "legal_violations": legal_violations,
            "category_summary": category_counts,
            "recommendations": recommendations
        }
    
    def comprehensive_analysis(self, text: str) -> Tuple[Dict, List[Dict]]:
        """ì¢…í•© ë¶„ì„ (ê°œì¸ì •ë³´ë³´í˜¸ë²• ê¸°ë°˜)"""
        logger.info("ë¶„ì„ ì‹œì‘ - ê°œì¸ì •ë³´ë³´í˜¸ë²• ê¸°ë°˜ ë¶„ì„")
        self._emit_status("ğŸ” ì •ê·œì‹ ê¸°ë°˜ ê°œì¸ì •ë³´ íƒì§€ ì¤‘...")
        
        # 1ë‹¨ê³„: ì •ê·œì‹ ê¸°ë°˜ íƒì§€
        regex_detected = self.detect_sensitive_info_regex(text)
        logger.info(f"ì •ê·œì‹ íƒì§€ ì™„ë£Œ: {len(regex_detected)}ê°œ")
        self._emit_status(f"âœ… ì •ê·œì‹ íƒì§€: {len(regex_detected)}ê°œ")
        
        # 2ë‹¨ê³„: ë¯¼ê°ì •ë³´ í‚¤ì›Œë“œ íƒì§€
        self._emit_status("ğŸ” ë¯¼ê°ì •ë³´ í‚¤ì›Œë“œ íƒì§€ ì¤‘...")
        keyword_detected = self.detect_sensitive_keywords(text)
        logger.info(f"í‚¤ì›Œë“œ íƒì§€ ì™„ë£Œ: {len(keyword_detected)}ê°œ")
        self._emit_status(f"âœ… í‚¤ì›Œë“œ íƒì§€: {len(keyword_detected)}ê°œ")
        
        # 3ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ ë¶„ì„
        self._emit_status("ğŸ“Š ê·œì¹™ ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ì„ ì¤‘...")
        rule_based_analysis = self._create_enhanced_analysis(text)
        logger.info("ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ")
        self._emit_status("âœ… ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ")
        
        # 4ë‹¨ê³„: LLM ë¶„ì„ (ì„ íƒì )
        llm_enhanced = False
        try:
            logger.info("LLM ë¶„ì„ ì‹œë„ ì¤‘...")
            self._emit_status("ğŸ¤– LLM ë¶„ì„ ì‹œë„ ì¤‘...")
            llm_analysis = self.analyze_with_llm(text)
            
            if llm_analysis and 'risk_level' in llm_analysis:
                rule_based_analysis = llm_analysis
                llm_enhanced = True
                logger.info("LLM ë¶„ì„ ê²°ê³¼ ì ìš©")
                self._emit_status("âœ… LLM ë¶„ì„ ì„±ê³µ")
            else:
                self._emit_status("âš ï¸ LLM ê²°ê³¼ ë¬´íš¨ - ê·œì¹™ ê¸°ë°˜ ê²°ê³¼ ì‚¬ìš©")
        except Exception as e:
            logger.warning(f"LLM ë¶„ì„ ì˜ˆì™¸: {str(e)}")
            self._emit_status("âŒ LLM ë¶„ì„ ì‹¤íŒ¨ - ê·œì¹™ ê¸°ë°˜ ê²°ê³¼ ì‚¬ìš©")
        
        # 5ë‹¨ê³„: ê¶Œê³ ì‚¬í•­ ë³´ì¥
        if len(rule_based_analysis.get('recommendations', [])) < 3:
            all_detected = regex_detected + [
                k for k in keyword_detected 
                if not any(self._is_overlapping(k['start'], k['end'], r['start'], r['end']) 
                          for r in regex_detected)
            ]
            enhanced_recommendations = self.recommendation_engine.generate_recommendations(
                all_detected,
                rule_based_analysis.get('risk_level', 'ë³´í†µ'),
                rule_based_analysis.get('risk_score', 50),
                text
            )
            rule_based_analysis['recommendations'] = enhanced_recommendations
        
        # 6ë‹¨ê³„: íƒì§€ í•­ëª© í†µí•©
        all_detected = regex_detected.copy()
        existing_ranges = [(d['start'], d['end']) for d in regex_detected]
        
        for kw in keyword_detected:
            is_dup = any(
                self._is_overlapping(kw['start'], kw['end'], s, e)
                for s, e in existing_ranges
            )
            if not is_dup:
                all_detected.append(kw)
        
        if llm_enhanced:
            for llm_item in rule_based_analysis.get('detected_info', []):
                value = llm_item.get('value', '')
                if value and not any(d['value'] == value for d in all_detected):
                    pos = text.find(value)
                    if pos != -1:
                        all_detected.append({
                            'type': llm_item.get('type', 'ê¸°íƒ€'),
                            'value': value,
                            'start': pos,
                            'end': pos + len(value),
                            'context': llm_item.get('context', 'LLM íƒì§€'),
                            'method': 'llm',
                            'legal_category': llm_item.get('legal_category', 'ì¼ë°˜ê°œì¸ì •ë³´')
                        })
        
        all_detected.sort(key=lambda x: x.get('start', 0))
        
        analysis_method = "ê·œì¹™ ê¸°ë°˜ + LLM" if llm_enhanced else "ê·œì¹™ ê¸°ë°˜"
        logger.info(f"ë¶„ì„ ì™„ë£Œ ({analysis_method}): {len(all_detected)}ê°œ í•­ëª©")
        
        return rule_based_analysis, all_detected
    
    def mask_sensitive_info(self, text: str, detected_items: List[Dict]) -> str:
        """ë¯¼ê°ì •ë³´ ë§ˆìŠ¤í‚¹"""
        masked_text = text
        offset = 0
        
        for item in sorted(detected_items, key=lambda x: x.get('start', 0)):
            start = item.get('start', 0) + offset
            end = item.get('end', 0) + offset
            value = item.get('value', '')
            info_type = item['type']
            
            if value:
                mask_char = '*'
                
                # ê³ ìœ ì‹ë³„ì •ë³´ëŠ” ì• 4ìë¦¬ë§Œ í‘œì‹œ
                if info_type in ['ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸', 'ì—¬ê¶Œë²ˆí˜¸', 'ìš´ì „ë©´í—ˆë²ˆí˜¸', 'ì™¸êµ­ì¸ë“±ë¡ë²ˆí˜¸']:
                    masked = value[:4] + mask_char * (len(value) - 4)
                
                # ê¸ˆìœµì •ë³´ëŠ” ì• 4ìë¦¬ë§Œ í‘œì‹œ
                elif info_type in ['ì¹´ë“œë²ˆí˜¸', 'ê³„ì¢Œë²ˆí˜¸']:
                    masked = value[:4] + mask_char * (len(value) - 4)
                
                # ì „í™”ë²ˆí˜¸ëŠ” ì¤‘ê°„ ë§ˆìŠ¤í‚¹
                elif info_type in ['ì „í™”ë²ˆí˜¸', 'íœ´ëŒ€ì „í™”']:
                    parts = value.split('-')
                    if len(parts) == 3:
                        masked = f"{parts[0]}-{mask_char * len(parts[1])}-{parts[2]}"
                    else:
                        masked = mask_char * len(value)
                
                # ì´ë©”ì¼ì€ ì•„ì´ë”” ì²« ê¸€ìë§Œ í‘œì‹œ
                elif info_type == 'ì´ë©”ì¼':
                    at_pos = value.find('@')
                    if at_pos > 0:
                        masked = value[0] + mask_char * (at_pos - 1) + value[at_pos:]
                    else:
                        masked = mask_char * len(value)
                
                # ê·¸ ì™¸ëŠ” ì „ì²´ ë§ˆìŠ¤í‚¹
                else:
                    masked = mask_char * len(value)
                
                masked_text = masked_text[:start] + masked + masked_text[end:]
                offset += len(masked) - len(value)
        
        return masked_text
    
    def get_legal_summary(self, detected_items: List[Dict]) -> Dict:
        """ë²•ì  ë¶„ë¥˜ë³„ ìš”ì•½ ìƒì„±"""
        summary = {
            "ê³ ìœ ì‹ë³„ì •ë³´": {
                "count": 0,
                "items": [],
                "legal_basis": "ì œ24ì¡° (ê³ ìœ ì‹ë³„ì •ë³´ì˜ ì²˜ë¦¬ ì œí•œ)",
                "requirement": "ì²˜ë¦¬ ì œí•œ, ì•”í˜¸í™” ì˜ë¬´, ë³„ë„ ë™ì˜ í•„ìš”"
            },
            "ë¯¼ê°ì •ë³´": {
                "count": 0,
                "items": [],
                "legal_basis": "ì œ23ì¡° (ë¯¼ê°ì •ë³´ì˜ ì²˜ë¦¬ ì œí•œ)",
                "requirement": "ì›ì¹™ì  ì²˜ë¦¬ ê¸ˆì§€, ë³„ë„ ë™ì˜ í•„ìš”"
            },
            "ê¸ˆìœµì •ë³´": {
                "count": 0,
                "items": [],
                "legal_basis": "ì œ34ì¡°ì˜2 (ë…¸ì¶œëœ ê°œì¸ì •ë³´ì˜ ì‚­ì œÂ·ì°¨ë‹¨)",
                "requirement": "ì •ë³´í†µì‹ ë§ ë…¸ì¶œ ê¸ˆì§€"
            },
            "ì¼ë°˜ê°œì¸ì •ë³´": {
                "count": 0,
                "items": [],
                "legal_basis": "ì œ2ì¡° (ì •ì˜)",
                "requirement": "ê°œì¸ì •ë³´ ì²˜ë¦¬ ê¸°ë³¸ ì›ì¹™ ì ìš©"
            }
        }
        
        for item in detected_items:
            category = item.get('legal_category', 'ì¼ë°˜ê°œì¸ì •ë³´')
            if category in summary:
                summary[category]["count"] += 1
                summary[category]["items"].append({
                    "type": item['type'],
                    "value": item['value'][:20] + "..." if len(item['value']) > 20 else item['value']
                })
        
        return summary
